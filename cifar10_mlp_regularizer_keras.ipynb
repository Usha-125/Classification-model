{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/fCZKoshqhtwiHNG/SgPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Usha-125/Classification-model/blob/main/cifar10_mlp_regularizer_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDZEkJM_-z6B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment  with depth(number of layers)\n",
        "\n",
        "model 1\n",
        "build a model with one hidden layer train it and find validation accuracy (128)\n",
        "\n",
        "model 2\n",
        "build a model with 3 hidden layers and find validation accuracy(1024,512,256)\n",
        "\n",
        "model 3\n",
        "build a model with 5 hidden layers and find validationaccuracy.(1024,512,256,128,64)\n",
        "\n",
        "plot the vaidation accuracy for all 3 models\n",
        "\n",
        "plot the vaidation loss for all 3 models\n",
        "in a single plot"
      ],
      "metadata": {
        "id": "fMjhWD3e-1c0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment  with depth(number of layers)\n",
        "\n",
        "model 1\n",
        "build a model with SDG optimizer,train it and find validation accuracy (SDG)\n",
        "\n",
        "model 2\n",
        "build a model with adam optimzer and find validation accuracy(adam optimizer)\n",
        "\n",
        "model 3\n",
        "build a model with RMSprop and find validationaccuracy.(RMSprop optimizer)\n",
        "\n",
        "plot the vaidation accuracy for all 3 models\n",
        "\n",
        "plot the vaidation loss for all 3 models\n",
        "in a single plot\n",
        " noteLuse different model name for each for the model"
      ],
      "metadata": {
        "id": "rWfmC9KZAzxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J6S8KoB3ATVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing modules and libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.datasets import cifar100\n",
        "from keras.utils import to_categorical\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loading CIFAR-100 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 100)\n",
        "y_test = to_categorical(y_test, 100)\n",
        "\n",
        "# --------------------------\n",
        "# Base Model\n",
        "# --------------------------\n",
        "model_base = Sequential([\n",
        "    Flatten(input_shape=(32, 32, 3)),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model_base.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train\n",
        "history_base = model_base.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "loss_base, test_accuracy_base = model_base.evaluate(x_test, y_test)\n",
        "print(f\"Base Model Test Accuracy: {test_accuracy_base:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# Model 2: With L2 Regularizer (1e-4) + Dropout\n",
        "# --------------------------\n",
        "model_l2 = Sequential([\n",
        "    Flatten(input_shape=(32, 32, 3)),\n",
        "    Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "    Dropout(0.3),\n",
        "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model_l2.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train\n",
        "history_l2 = model_l2.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "loss_l2, test_accuracy_l2 = model_l2.evaluate(x_test, y_test)\n",
        "print(f\"L2 Regularized Model Test Accuracy: {test_accuracy_l2:.4f}\")\n",
        "\n",
        "\n",
        "#**************************************************************************************\n",
        "# --------------------------\n",
        "# Model 3: With L2 Regularizer (1e-4) + Dropout\n",
        "# --------------------------\n",
        "model_l2 = Sequential([\n",
        "    Flatten(input_shape=(32, 32, 3)),\n",
        "    Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(1e-2)),\n",
        "    Dropout(0.3),\n",
        "    Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-2)),\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-2)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model_l2.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train\n",
        "history_l2 = model_l2.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "loss_l2, test_accuracy_l2 = model_l2.evaluate(x_test, y_test)\n",
        "print(f\"L2 Regularized Model Test Accuracy: {test_accuracy_l2:.4f}\")\n",
        "\n",
        "#visualization\n",
        "plt.plot(history.history['val_accurcay'],label='without regularizer',color=red)\n",
        "plt.plot(history_model2.history['val_accurcay'],label='le4',color=blue)\n",
        "plt.plot(history_model3.history['val_accurcay'],label='le2',color=blue)\n",
        "plt.tilte='validation acuracy'\n",
        "plt.xlable='epochs'\n",
        "plt.xlable=''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh96TnZ3_DrX",
        "outputId": "08a9fbd0-6f5a-4932-b94b-6ee5b8f4b565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 114ms/step - accuracy: 0.0119 - loss: 4.6094 - val_accuracy: 0.0257 - val_loss: 4.4077\n",
            "Epoch 2/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 93ms/step - accuracy: 0.0424 - loss: 4.2603 - val_accuracy: 0.0671 - val_loss: 4.0413\n",
            "Epoch 3/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 95ms/step - accuracy: 0.0775 - loss: 3.9795 - val_accuracy: 0.0949 - val_loss: 3.9007\n",
            "Epoch 4/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 96ms/step - accuracy: 0.1005 - loss: 3.8608 - val_accuracy: 0.1081 - val_loss: 3.8145\n",
            "Epoch 5/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 94ms/step - accuracy: 0.1215 - loss: 3.7390 - val_accuracy: 0.1241 - val_loss: 3.7513\n",
            "Epoch 6/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 94ms/step - accuracy: 0.1393 - loss: 3.6293 - val_accuracy: 0.1462 - val_loss: 3.6321\n",
            "Epoch 7/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 94ms/step - accuracy: 0.1565 - loss: 3.5396 - val_accuracy: 0.1540 - val_loss: 3.5707\n",
            "Epoch 8/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 96ms/step - accuracy: 0.1671 - loss: 3.4649 - val_accuracy: 0.1666 - val_loss: 3.5139\n",
            "Epoch 9/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 95ms/step - accuracy: 0.1781 - loss: 3.4054 - val_accuracy: 0.1667 - val_loss: 3.4999\n",
            "Epoch 10/10\n",
            "\u001b[1m218/313\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 0.1906 - loss: 3.3259"
          ]
        }
      ]
    }
  ]
}